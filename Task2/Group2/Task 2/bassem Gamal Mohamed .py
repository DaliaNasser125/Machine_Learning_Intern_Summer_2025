# -*- coding: utf-8 -*-
"""task2_cellule.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13BjdzBD8HhRq13WIv7zQSMJLs3rluZkh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/first inten project.csv')
data.head()

data['date of reservation'].unique()

# Fix the specific invalid date
data['date of reservation'] = data['date of reservation'].replace('2018-2-29', '3/1/2018')

# 1. Check for missing (null) values
print("NULL Values:")
print(data.isnull().sum())
print("\n")

# 2. Check data types
print("Data Types:")
print(data.dtypes)
print("\n")



# Convert 'date of reservation' to datetime format
data['date of reservation'] = pd.to_datetime(data['date of reservation'], format='%m/%d/%Y', errors='coerce')

# Verify the change
print(data['date of reservation'].dtypes)
print(data['date of reservation'].head())

# 1. Check for missing (null) values
print("NULL Values:")
print(data.isnull().sum())
print("\n")

# Strip white spaces from column names
data.columns = data.columns.str.strip()

# Strip white spaces from all object/string columns
for col in data.select_dtypes(include='object').columns:
    data[col] = data[col].str.strip()
# Show a summary after cleaning
print("Data Sample After Cleaning:")
print(data.head())

# Check unique values in each categorical column to confirm cleanup
for col in data.select_dtypes(include='object').columns:
    print(f"\nUnique values in '{col}':")
    print(data[col].unique())

# Step 1: Select numeric columns only
numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns
print("Numeric columns:")
print(numeric_cols)

# Step 2: Detect outliers using IQR
outliers_iqr = {}

for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Count how many outliers exist
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]

    outliers_iqr[col] = len(outliers)
    print(f"{col}: {len(outliers)} outliers")

numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns

outlier_percentages = {}

for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Find outliers for this column
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    outlier_count = len(outliers)

    # Compute outlier percentage for this column
    outlier_percentage = (outlier_count / len(data)) * 100
    outlier_percentages[col] = round(outlier_percentage, 2)

# Show percentages
print("Outlier Percentage for Each Numeric Column:")
for col, perc in outlier_percentages.items():
    print(f"{col}: {perc}%")

# Make a copy of the original data
data_no_outliers = data.copy()

# Loop through each numeric column and remove only the rows with outliers in that column
for col in numeric_cols:
    Q1 = data_no_outliers[col].quantile(0.25)
    Q3 = data_no_outliers[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove outliers for the current column only
    condition = (data_no_outliers[col] >= lower_bound) & (data_no_outliers[col] <= upper_bound)
    data_no_outliers = data_no_outliers[condition]

# Print final shape
print("Original shape:", data.shape)
print("Shape after outlier removal:", data_no_outliers.shape)

# Step 4: Fix outliers by capping them to the IQR range
for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap outliers to the boundaries
    data[col] = data[col].apply(lambda x: lower_bound if x < lower_bound else upper_bound if x > upper_bound else x)

print("Outliers have been capped using IQR method.")

# Step 1.1: Encode target column for correlation
data['booking status'] = data['booking status'].map({'Canceled': 1, 'Not_Canceled': 0})

# Step 1.2: Calculate correlation with target
correlations = data.corr(numeric_only=True)['booking status'].sort_values(ascending=False)

print("Correlation with booking status:")
print(correlations)

selected_features = [
    'lead time',
    'average price',
    'special requests'
]

# Step 1: Create total nights
data['total nights'] = data['number of weekend nights'] + data['number of week nights']
# Avoid division by zero
data['price_per_adult'] = data['average price'] / data['number of adults'].replace(0, 1)

# Step 3: Create boolean feature for special requests
data['has_special_requests'] = data['special requests'].apply(lambda x: 1 if x > 0 else 0)

# Step 4: Create high price flag (based on median)
median_price = data['average price'].median()
data['high_price'] = data['average price'].apply(lambda x: 1 if x > median_price else 0)

data.drop('Booking_ID', axis=1, inplace=True)

# Step 1: Select categorical columns (object or category dtype)
categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()

print("Categorical Columns:")
print(categorical_cols)

print(data['market segment type'].unique())
print(data['type of meal'].unique())
print(data['room type'].unique())
print(data['booking status'].unique())

# One-Hot Encode the remaining categorical columns
data = pd.get_dummies(data, columns=['type of meal', 'room type', 'market segment type'], drop_first=True)

print("Categorical transformation complete. Data shape:", data.shape)

# Step 1.2: Calculate correlation with target
correlations = data.corr(numeric_only=True)['booking status'].sort_values(ascending=False)

print("Correlation with booking status:")
print(correlations)

columns_to_drop = [
    'room type_Room_Type 3',
    'room type_Room_Type 2',
    'type of meal_Meal Plan 3',
    'type of meal_Not Selected'
]
data.drop(columns=columns_to_drop, axis=1, inplace=True)

data.head()

# Separate features (X) and target (y)
X = data.drop('booking status', axis=1)
y = data['booking status']

# Remove datetime columns
X_filtered = X.select_dtypes(exclude=['datetime64[ns]'])

# Ø«Ù… Ø´ØºÙ„ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø¹ X_filtered
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X_filtered, y)

selected_features = X_filtered.columns[selector.get_support()]
print("Top features:", selected_features)

from sklearn.model_selection import train_test_split

X_selected = X_filtered[selected_features]
# Split into train and test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Data split complete:")
print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)

from sklearn.preprocessing import StandardScaler
X_selected = X_filtered[selected_features]
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
import pandas as pd

X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=selected_features)
print(X_train_scaled_df.head())

from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score

# Create and train the Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)

# Predict on training set
y_train_pred_lin = lin_reg.predict(X_train_scaled)
y_train_pred_lin_class = [1 if p >= 0.5 else 0 for p in y_train_pred_lin]

# Predict on test set
y_test_pred_lin = lin_reg.predict(X_test_scaled)
y_test_pred_lin_class = [1 if p >= 0.5 else 0 for p in y_test_pred_lin]

# Accuracy
train_acc_lin = accuracy_score(y_train, y_train_pred_lin_class)
test_acc_lin = accuracy_score(y_test, y_test_pred_lin_class)

# Print results
print("ðŸ”¹ Linear Regression Training Accuracy:", round(train_acc_lin, 4))
print("ðŸ”¹ Linear Regression Test Accuracy    :", round(test_acc_lin, 4))

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score

# Create pipeline: PolynomialFeatures + LinearRegression
poly_model = make_pipeline(
    PolynomialFeatures(degree=2),  # Degree 2 polynomial
    LinearRegression()
)

# Train the model
poly_model.fit(X_train_scaled, y_train)

# Predict on training data
y_train_pred_poly = poly_model.predict(X_train_scaled)
y_train_pred_poly_class = [1 if p >= 0.5 else 0 for p in y_train_pred_poly]

# Predict on test data
y_test_pred_poly = poly_model.predict(X_test_scaled)
y_test_pred_poly_class = [1 if p >= 0.5 else 0 for p in y_test_pred_poly]

# Accuracy scores
train_acc_poly = accuracy_score(y_train, y_train_pred_poly_class)
test_acc_poly = accuracy_score(y_test, y_test_pred_poly_class)

# Print results
print("ðŸ”¸ Polynomial Regression (degree=2) Training Accuracy:", round(train_acc_poly, 4))
print("ðŸ”¸ Polynomial Regression (degree=2) Test Accuracy    :", round(test_acc_poly, 4))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Create and train the Logistic Regression model
log_model = LogisticRegression(random_state=42)
log_model.fit(X_train_scaled, y_train)

# Make predictions on the training set
y_train_pred_log = log_model.predict(X_train_scaled)

# Make predictions on the test set
y_test_pred_log = log_model.predict(X_test_scaled)

# Evaluate the model
train_acc_log = accuracy_score(y_train, y_train_pred_log)
test_acc_log = accuracy_score(y_test, y_test_pred_log)

# Print accuracies
print("Logistic Regression Training Accuracy:", round(train_acc_log, 4))
print("Logistic Regression Test Accuracy    :", round(test_acc_log, 4))